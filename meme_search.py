import requests
import json
from googleapiclient.discovery import build
import pandas as pd
from datetime import datetime, timedelta
import streamlit as st
from io import BytesIO
from openpyxl.styles import Alignment, Font, PatternFill, Border, Side
from openpyxl.drawing.image import Image as OpxImage
import re
import time
import os
import zipfile
import yt_dlp
import random

# API í‚¤ë¥¼ st.secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°
SERPAPI_API_KEY = st.secrets["SERPAPI_API_KEY"]
YOUTUBE_API_KEYS = st.secrets["YOUTUBE_API_KEYS"]

def get_youtube_api_key():
    """YouTube API í‚¤ë¥¼ ìˆœí™˜í•˜ì—¬ ì‚¬ìš©"""
    return random.choice(YOUTUBE_API_KEYS)

# êµ­ê°€ë³„ ê²€ìƒ‰ì–´ ì„¤ì •
COUNTRY_QUERIES = {
    "í•œêµ­": {"term": "ë°ˆ", "geo": "KR", "hl": "ko"},
    "ë¯¸êµ­": {"term": "meme", "geo": "US", "hl": "en"},
    "ì¼ë³¸": {"term": "ãƒŸãƒ¼ãƒ ", "geo": "JP", "hl": "ja"}
}

# ê¸°ê°„ ì„¤ì •
PERIOD_OPTIONS = {
    "ìµœê·¼ 1ì¼": 1,
    "ìµœê·¼ 1ì£¼ì¼": 7,
    "ìµœê·¼ 1ë‹¬": 30,
    "ìµœê·¼ 1ë…„": 365
}

def parse_iso8601_duration(duration_str):
    """YouTube ë™ì˜ìƒ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜"""
    pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
    match = re.match(pattern, duration_str)
    hours = int(match.group(1)) if match and match.group(1) else 0
    minutes = int(match.group(2)) if match and match.group(2) else 0
    seconds = int(match.group(3)) if match and match.group(3) else 0
    return hours * 3600 + minutes * 60 + seconds

def get_youtube_shorts(search_query, days_ago=365, max_results=5):
    """YouTube Shorts ê²€ìƒ‰ í•¨ìˆ˜"""
    try:
        youtube = build('youtube', 'v3', developerKey=get_youtube_api_key())
        
        # ì„ íƒëœ ê¸°ê°„ ì „ ë‚ ì§œ ê³„ì‚°
        period_ago = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # ê²€ìƒ‰ ì‹¤í–‰
        search_response = youtube.search().list(
            q=f"{search_query} #shorts",
            part="id,snippet",
            maxResults=50,  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            type="video",
            videoDuration="short",
            order="viewCount",
            publishedAfter=period_ago
        ).execute()

        videos = []
        video_ids = [item['id']['videoId'] for item in search_response['items']]

        # ë¹„ë””ì˜¤ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        video_response = youtube.videos().list(
            part="snippet,statistics,contentDetails",
            id=','.join(video_ids)
        ).execute()

        for video in video_response['items']:
            duration = parse_iso8601_duration(video['contentDetails']['duration'])
            
            # 60ì´ˆ ì´í•˜ ë™ì˜ìƒë§Œ í•„í„°ë§
            if duration <= 60:
                video_info = {
                    'title': video['snippet']['title'],
                    'video_id': video['id'],
                    'view_count': int(video['statistics'].get('viewCount', 0)),
                    'like_count': int(video['statistics'].get('likeCount', 0)),
                    'published_at': video['snippet']['publishedAt'],
                    'channel_title': video['snippet']['channelTitle'],
                    'url': f'https://www.youtube.com/watch?v={video["id"]}'
                }
                videos.append(video_info)

        # ì¡°íšŒìˆ˜ ê¸°ì¤€ ìƒìœ„ 5ê°œ ë°˜í™˜
        return sorted(videos, key=lambda x: x['view_count'], reverse=True)[:max_results]

    except Exception as e:
        print(f"YouTube API ì˜¤ë¥˜: {str(e)}")
        return []

def main():
    # ì „ì²´ í™”ë©´ ëª¨ë“œ ì„¤ì •
    st.set_page_config(layout="wide")
    
    # íƒ€ì´í‹€ í‘œì‹œ
    st.title("ë°ˆ(Meme) íŠ¸ë Œë“œ ë° YouTube Shorts ë¶„ì„", anchor=False)
    
    # ë‹¤ì‹œ ì‹œì‘í•˜ê¸° ë²„íŠ¼ì„ íƒ€ì´í‹€ ì•„ë˜ì— ì™¼ìª½ ì •ë ¬ë¡œ ë°°ì¹˜
    col1, col2, col3 = st.columns([1, 4, 4])
    with col1:
        st.button("ğŸ”„ ë‹¤ì‹œ ì‹œì‘", on_click=lambda: [st.session_state.clear(), st.experimental_rerun()])
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'results' not in st.session_state:
        st.session_state.results = {}
    if 'youtube_results' not in st.session_state:
        st.session_state.youtube_results = {}
    if 'analysis_complete' not in st.session_state:
        st.session_state.analysis_complete = False
    
    # êµ­ê°€ë³„ êµ­ê¸° ì´ëª¨ì§€ ë§¤í•‘
    COUNTRY_FLAGS = {
        "í•œêµ­": "ğŸ‡°ğŸ‡·",
        "ë¯¸êµ­": "ğŸ‡ºğŸ‡¸",
        "ì¼ë³¸": "ğŸ‡¯ğŸ‡µ"
    }
    
    with st.expander("í”„ë¡œê·¸ë¨ ì„¤ëª…", expanded=True):
        st.markdown("""
        ì´ í”„ë¡œê·¸ë¨ì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
        1. ì„ íƒí•œ êµ­ê°€ì˜ ë°ˆ ê´€ë ¨ ê¸‰ìƒìŠ¹ ë°ˆ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘
        2. í•´ë‹¹ ë°ˆ í‚¤ì›Œë“œë¡œ YouTube Shorts ì˜ìƒì„ ê²€ìƒ‰
        3. ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
        """)

    # ì‚¬ì´ë“œë°”ì— ì„¤ì • ì˜µì…˜ ì¶”ê°€
    st.sidebar.header("ê²€ìƒ‰ ì„¤ì •")
    selected_countries = st.sidebar.multiselect(
        "êµ­ê°€ ì„ íƒ",
        options=list(COUNTRY_QUERIES.keys()),
        default=["í•œêµ­"]
    )
    
    selected_period = st.sidebar.selectbox(
        "ê²€ìƒ‰ ê¸°ê°„",
        options=list(PERIOD_OPTIONS.keys()),
        index=3  # ê¸°ë³¸ê°’: 1ë…„
    )

    if st.sidebar.button("ë¶„ì„ ì‹œì‘"):
        results = {}
        youtube_results = {}
        
        # ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì¤„ progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, country in enumerate(selected_countries):
            try:
                q = COUNTRY_QUERIES[country]
                status_text.text(f"ê²€ìƒ‰ ì¤‘: {q['term']} ({q['geo']})")
                
                # Google Trends API í˜¸ì¶œ
                params = {
                    "engine": "google_trends",
                    "q": q["term"],
                    "geo": q["geo"],
                    "hl": q["hl"],
                    "data_type": "RELATED_QUERIES",
                    "api_key": SERPAPI_API_KEY
                }
                
                response = requests.get("https://serpapi.com/search.json", params=params)
                response.raise_for_status()
                data = response.json()
                
                related_queries = data.get("related_queries", {})
                rising_queries = related_queries.get("rising", [])[:15] if related_queries.get("rising") else []
                
                results[country] = {
                    "rising_related_queries": [{
                        "query": item["query"],
                        "value": item["value"]
                    } for item in rising_queries]
                }
                
                # YouTube Shorts ê²€ìƒ‰
                status_text.text(f"YouTube Shorts ê²€ìƒ‰ ì¤‘... ({country})")
                youtube_results[country] = {}
                for item in rising_queries:
                    shorts_data = get_youtube_shorts(
                        item["query"],
                        days_ago=PERIOD_OPTIONS[selected_period]
                    )
                    youtube_results[country][item["query"]] = shorts_data
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ ({country}): {str(e)}")
                results[country] = {"error": str(e)}
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress((idx + 1) / len(selected_countries))
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.results = results
        st.session_state.youtube_results = youtube_results
        st.session_state.analysis_complete = True
        
        status_text.text("ë¶„ì„ ì™„ë£Œ!")
        
        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€ ë°”ë¡œ ì•„ë˜ì— ë°°ì¹˜
        try:
            excel_buffer = BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                # YouTube Shorts ê²°ê³¼ ì‹œíŠ¸ (ë¨¼ì € ì‘ì„±)
                youtube_data = []
                for country, queries in st.session_state.youtube_results.items():
                    for query, shorts in queries.items():
                        for short in shorts:
                            # ì¸ë„¤ì¼ URL ê°€ì ¸ì˜¤ê¸°
                            try:
                                thumbnail_url = f"https://i.ytimg.com/vi/{short['video_id']}/hqdefault.jpg"
                            except:
                                thumbnail_url = ""

                            youtube_data.append({
                                "Country": country,
                                "Search Query": query,
                                "Title": short["title"],
                                "Channel": short["channel_title"],
                                "Views": short["view_count"],
                                "Likes": short["like_count"],
                                "Published Date": datetime.strptime(short["published_at"], "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d"),
                                "url": short["url"],
                                "thumbnail_url": thumbnail_url
                            })
                
                df_youtube = pd.DataFrame(youtube_data)
                df_youtube.to_excel(writer, sheet_name='YouTube Shorts', index=False)

                # Google Trends ê²°ê³¼ ì‹œíŠ¸ (ë‚˜ì¤‘ì— ì‘ì„±)
                trends_data = []
                for country, data in st.session_state.results.items():
                    for query in data.get("rising_related_queries", []):
                        trends_data.append({
                            "Country": country,
                            "Related Query": query["query"],
                            "Value": query["value"]
                        })
                
                df_trends = pd.DataFrame(trends_data)
                df_trends.to_excel(writer, sheet_name='Meme Keyword', index=False)

                # ìŠ¤íƒ€ì¼ ì ìš©
                workbook = writer.book
                for sheet_name in ['YouTube Shorts', 'Meme Keyword']:
                    worksheet = writer.sheets[sheet_name]
                    
                    # í—¤ë” ìŠ¤íƒ€ì¼
                    header_fill = PatternFill(start_color="CCFFCC", end_color="CCFFCC", fill_type="solid")
                    header_font = Font(bold=True)
                    
                    for cell in worksheet[1]:
                        cell.fill = header_fill
                        cell.font = header_font
                    
                    # ì „ì²´ ì…€ ì¤‘ì•™ ì •ë ¬
                    for row in worksheet.iter_rows():
                        for cell in row:
                            cell.alignment = Alignment(horizontal="center", vertical="center")

                    # ì—´ ë„ˆë¹„ ì¡°ì •
                    if sheet_name == 'YouTube Shorts':
                        # ê¸°ë³¸ ì—´ ë„ˆë¹„ ì„¤ì •
                        for col_letter in ["A", "B"]:  # Country, Search Query
                            worksheet.column_dimensions[col_letter].width = 15
                        worksheet.column_dimensions["C"].width = 30  # Title
                        worksheet.column_dimensions["D"].width = 20  # Channel
                        for col_letter in ["E", "F"]:  # Views, Likes
                            worksheet.column_dimensions[col_letter].width = 12
                        worksheet.column_dimensions["G"].width = 15  # Published Date
                        worksheet.column_dimensions["H"].width = 7   # URL
                        worksheet.column_dimensions["I"].width = 30  # Thumbnail

                        # í–‰ ë†’ì´ ì¡°ì •
                        worksheet.row_dimensions[1].height = 30
                        for row_idx in range(2, len(df_youtube) + 2):
                            worksheet.row_dimensions[row_idx].height = 135

                        # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì‚½ì…
                        for i, row in df_youtube.iterrows():
                            try:
                                cell_row = i + 2  # 2í–‰ë¶€í„° ì‹œì‘
                                thumb_url = row["thumbnail_url"]
                                if thumb_url and isinstance(thumb_url, str):
                                    resp = requests.get(thumb_url, timeout=5)
                                    if resp.status_code == 200:
                                        img_data = BytesIO(resp.content)
                                        img = OpxImage(img_data)
                                        img.width = 240
                                        img.height = 180
                                        worksheet.add_image(img, f"I{cell_row}")
                            except Exception as e:
                                st.warning(f"ì´ë¯¸ì§€ ì‚½ì… ì‹¤íŒ¨ (í–‰ {cell_row}): {str(e)}")
                                continue  # ì´ë¯¸ì§€ ì‚½ì… ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°

                        # URL ì—´ì— í•˜ì´í¼ë§í¬ ì„¤ì •
                        for row_idx in range(2, len(df_youtube) + 2):
                            try:
                                cell = worksheet[f"H{row_idx}"]
                                if cell.value and isinstance(cell.value, str) and cell.value.startswith("http"):
                                    cell.hyperlink = cell.value
                                    cell.style = "Hyperlink"
                                    cell.alignment = Alignment(horizontal="left", vertical="center", wrapText=True)
                            except Exception as e:
                                continue

                        # íŠ¹ì • ì—´ì— ì¤„ë°”ê¿ˆ ì ìš©
                        for col_letter in ["C", "D", "I"]:  # Title, Channel, Thumbnail
                            for row_idx in range(1, worksheet.max_row + 1):
                                try:
                                    cell = worksheet[f"{col_letter}{row_idx}"]
                                    current_alignment = cell.alignment
                                    new_alignment = Alignment(
                                        horizontal=current_alignment.horizontal if current_alignment else "center",
                                        vertical=current_alignment.vertical if current_alignment else "center",
                                        wrapText=True
                                    )
                                    cell.alignment = new_alignment
                                except Exception as e:
                                    continue

            # ì›Œí¬ë¶ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì‘ì—…ì´ with ë¸”ë¡ ì•ˆì—ì„œ ì™„ë£Œë¨
            # (ëª…ì‹œì ì¸ saveë‚˜ close í˜¸ì¶œ ì œê±°)

            st.download_button(
                label="ê²°ê³¼ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=excel_buffer.getvalue(),
                file_name="meme_search_results.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ê²°ê³¼ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
    if st.session_state.analysis_complete:
        # ê° êµ­ê°€ë³„ ê²°ê³¼ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜
        for country in st.session_state.results:
            st.header(f"{COUNTRY_FLAGS[country]} {country}")
            
            # ë‘ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ë¹„ìœ¨ ì¡°ì •: 1.2:2.8)
            col1, col2 = st.columns([1.2, 2.8])
            
            # ì™¼ìª½ ì»¬ëŸ¼: Google Trends ê²°ê³¼
            with col1:
                st.subheader("ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´")
                trends_df = pd.DataFrame(st.session_state.results[country]["rising_related_queries"])
                if not trends_df.empty:
                    st.dataframe(trends_df, height=600, use_container_width=True)  # ì»¨í…Œì´ë„ˆ ë„ˆë¹„ ì‚¬ìš©
                else:
                    st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì˜¤ë¥¸ìª½ ì»¬ëŸ¼: YouTube Shorts ê²°ê³¼
            with col2:
                st.subheader("YouTube Shorts ê²°ê³¼")
                if country in st.session_state.youtube_results:
                    tabs = st.tabs(list(st.session_state.youtube_results[country].keys()))
                    for tab, (query, shorts) in zip(tabs, st.session_state.youtube_results[country].items()):
                        with tab:
                            if shorts:
                                shorts_df = pd.DataFrame(shorts)
                                st.write(f"**í‚¤ì›Œë“œ: {query}**")
                                st.dataframe(
                                    shorts_df[['title', 'view_count', 'like_count', 'channel_title', 'url']],
                                    height=500,
                                    use_container_width=True  # ì»¨í…Œì´ë„ˆ ë„ˆë¹„ ì‚¬ìš©
                                )
                            else:
                                st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # êµ¬ë¶„ì„  ì œê±°
            st.write("")  # ê°„ê²©ì„ ìœ„í•œ ë¹ˆ ì¤„ ì¶”ê°€

    # ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ì¶”ê°€
    st.divider()
    st.header("ğŸ“¥ YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ")
    
    with st.expander("ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ", expanded=True):
        st.markdown("""
        1. ìœ íŠœë¸Œ urlì´ í¬í•¨ëœ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (`url` ì»¬ëŸ¼ í•„ìš”)
        2. "ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹œì‘" ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ì˜ìƒë“¤ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤
        3. ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ë©´ ZIP íŒŒì¼ë¡œ ì œê³µë©ë‹ˆë‹¤
        """)
    
    uploaded_video_file = st.file_uploader("ìœ íŠœë¸Œ urlì´ í¬í•¨ëœ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"], key="video_urls")
    
    if uploaded_video_file is not None:
        df_urls = pd.read_excel(uploaded_video_file)
        
        if "url" not in df_urls.columns:
            st.error("ì—‘ì…€ íŒŒì¼ì— 'url' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            if st.button("ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹œì‘"):
                st.info("ì˜ìƒì„ ë‹¤ìš´ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì„ì‹œ í´ë” ìƒì„±
                timestamp = int(time.time())
                download_dir = f"downloaded_videos_{timestamp}"
                os.makedirs(download_dir, exist_ok=True)
                
                # URL ëª©ë¡ ì¤€ë¹„
                video_links = []
                for i, row in df_urls.iterrows():
                    url = row["url"]
                    if pd.isna(url) or not isinstance(url, str):
                        continue
                    if "youtube.com/shorts/" in url:
                        video_id = url.split("/")[-1]
                        url = f"https://www.youtube.com/watch?v={video_id}"
                    video_links.append(url)
                
                num_videos = len(video_links)
                st.write(f"ì´ {num_videos}ê°œì˜ ì˜ìƒì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
                
                # yt-dlp ì˜µì…˜ ì„¤ì •
                ydl_opts = {
                    'format': 'bestvideo[ext=mp4][vcodec^=avc]+bestaudio[ext=m4a]/best[ext=mp4]',
                    'outtmpl': os.path.join(download_dir, '%(title)s.%(ext)s'),
                    'merge_output_format': 'mp4',
                    'http_headers': {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'
                    },
                    'force-ipv4': True,
                    'postprocessors': [{
                        'key': 'FFmpegVideoConvertor',
                        'preferedformat': 'mp4'
                    }],
                }
                
                # ë‹¤ìš´ë¡œë“œ ì§„í–‰
                failed_list = []
                downloaded_count = 0
                progress_bar = st.progress(0)
                
                for idx, link in enumerate(video_links, start=1):
                    try:
                        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                            ydl.download([link])
                        downloaded_count += 1
                    except Exception as e:
                        failed_list.append({"url": link, "error_msg": str(e)})
                    
                    progress_percent = int(idx / num_videos * 100)
                    progress_bar.progress(progress_percent)
                
                st.success(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ì´ {num_videos}ê°œ ì¤‘ {downloaded_count}ê°œ ì„±ê³µ")
                
                # ì‹¤íŒ¨ ëª©ë¡ í‘œì‹œ
                if failed_list:
                    st.warning(f"{len(failed_list)}ê°œ ì˜ìƒì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    df_failed = pd.DataFrame(failed_list)
                    with st.expander("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ëª©ë¡"):
                        st.dataframe(df_failed)
                
                # ZIP íŒŒì¼ ìƒì„±
                zip_file_name = f"youtube_videos_{timestamp}.zip"
                with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(download_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            zipf.write(file_path, arcname=file)
                
                # ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                with open(zip_file_name, "rb") as f:
                    st.download_button(
                        label="ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=f,
                        file_name=zip_file_name,
                        mime="application/zip"
                    )
                
                # ì„ì‹œ íŒŒì¼/í´ë” ì •ë¦¬
                try:
                    os.remove(zip_file_name)
                    for f_name in os.listdir(download_dir):
                        os.remove(os.path.join(download_dir, f_name))
                    os.rmdir(download_dir)
                except:
                    pass

if __name__ == "__main__":
    main()
